{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8b715375",
   "metadata": {
    "id": "8b715375"
   },
   "source": [
    "# PINECONE\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9d615a77",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9d615a77",
    "outputId": "1df18135-0551-4376-f7ac-ea12ba7c8ce4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: langchain in c:\\users\\hachichamed\\anaconda3\\lib\\site-packages (0.1.13)\n",
      "Requirement already satisfied: PyYAML>=5.3 in c:\\users\\hachichamed\\anaconda3\\lib\\site-packages (from langchain) (6.0)\n",
      "Requirement already satisfied: SQLAlchemy<3,>=1.4 in c:\\users\\hachichamed\\anaconda3\\lib\\site-packages (from langchain) (1.4.39)\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in c:\\users\\hachichamed\\anaconda3\\lib\\site-packages (from langchain) (3.8.5)\n",
      "Requirement already satisfied: dataclasses-json<0.7,>=0.5.7 in c:\\users\\hachichamed\\anaconda3\\lib\\site-packages (from langchain) (0.6.4)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in c:\\users\\hachichamed\\anaconda3\\lib\\site-packages (from langchain) (1.33)\n",
      "Requirement already satisfied: langchain-community<0.1,>=0.0.29 in c:\\users\\hachichamed\\anaconda3\\lib\\site-packages (from langchain) (0.0.29)\n",
      "Requirement already satisfied: langchain-core<0.2.0,>=0.1.33 in c:\\users\\hachichamed\\anaconda3\\lib\\site-packages (from langchain) (0.1.33)\n",
      "Requirement already satisfied: langchain-text-splitters<0.1,>=0.0.1 in c:\\users\\hachichamed\\anaconda3\\lib\\site-packages (from langchain) (0.0.1)\n",
      "Requirement already satisfied: langsmith<0.2.0,>=0.1.17 in c:\\users\\hachichamed\\anaconda3\\lib\\site-packages (from langchain) (0.1.31)\n",
      "Requirement already satisfied: numpy<2,>=1 in c:\\users\\hachichamed\\anaconda3\\lib\\site-packages (from langchain) (1.24.3)\n",
      "Requirement already satisfied: pydantic<3,>=1 in c:\\users\\hachichamed\\anaconda3\\lib\\site-packages (from langchain) (1.10.8)\n",
      "Requirement already satisfied: requests<3,>=2 in c:\\users\\hachichamed\\anaconda3\\lib\\site-packages (from langchain) (2.31.0)\n",
      "Requirement already satisfied: tenacity<9.0.0,>=8.1.0 in c:\\users\\hachichamed\\anaconda3\\lib\\site-packages (from langchain) (8.2.2)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\hachichamed\\anaconda3\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (22.1.0)\n",
      "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in c:\\users\\hachichamed\\anaconda3\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (2.0.4)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\hachichamed\\anaconda3\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (6.0.2)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in c:\\users\\hachichamed\\anaconda3\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (4.0.2)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in c:\\users\\hachichamed\\anaconda3\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.8.1)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\hachichamed\\anaconda3\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.3.3)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in c:\\users\\hachichamed\\anaconda3\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.2.0)\n",
      "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in c:\\users\\hachichamed\\anaconda3\\lib\\site-packages (from dataclasses-json<0.7,>=0.5.7->langchain) (3.21.1)\n",
      "Requirement already satisfied: typing-inspect<1,>=0.4.0 in c:\\users\\hachichamed\\anaconda3\\lib\\site-packages (from dataclasses-json<0.7,>=0.5.7->langchain) (0.9.0)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in c:\\users\\hachichamed\\anaconda3\\lib\\site-packages (from jsonpatch<2.0,>=1.33->langchain) (2.1)\n",
      "Requirement already satisfied: anyio<5,>=3 in c:\\users\\hachichamed\\anaconda3\\lib\\site-packages (from langchain-core<0.2.0,>=0.1.33->langchain) (3.5.0)\n",
      "Requirement already satisfied: packaging<24.0,>=23.2 in c:\\users\\hachichamed\\anaconda3\\lib\\site-packages (from langchain-core<0.2.0,>=0.1.33->langchain) (23.2)\n",
      "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in c:\\users\\hachichamed\\anaconda3\\lib\\site-packages (from langsmith<0.2.0,>=0.1.17->langchain) (3.9.15)\n",
      "Requirement already satisfied: typing-extensions>=4.2.0 in c:\\users\\hachichamed\\anaconda3\\lib\\site-packages (from pydantic<3,>=1->langchain) (4.10.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\hachichamed\\anaconda3\\lib\\site-packages (from requests<3,>=2->langchain) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\hachichamed\\anaconda3\\lib\\site-packages (from requests<3,>=2->langchain) (1.26.16)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\hachichamed\\anaconda3\\lib\\site-packages (from requests<3,>=2->langchain) (2023.7.22)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in c:\\users\\hachichamed\\anaconda3\\lib\\site-packages (from SQLAlchemy<3,>=1.4->langchain) (2.0.1)\n",
      "Requirement already satisfied: sniffio>=1.1 in c:\\users\\hachichamed\\anaconda3\\lib\\site-packages (from anyio<5,>=3->langchain-core<0.2.0,>=0.1.33->langchain) (1.2.0)\n",
      "Requirement already satisfied: mypy-extensions>=0.3.0 in c:\\users\\hachichamed\\anaconda3\\lib\\site-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain) (1.0.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install langchain --upgrade\n",
    "# Version: 0.0.164"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "QnLdk1ZunjmU",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "QnLdk1ZunjmU",
    "outputId": "4fa4525a-c4e4-4494-8401-0c4387e0d09f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: python-dotenv in c:\\users\\hachichamed\\anaconda3\\lib\\site-packages (0.21.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install python-dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a9a92420",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pypdf\n",
      "  Obtaining dependency information for pypdf from https://files.pythonhosted.org/packages/b8/1e/071b6684ee2b299a74a0bcdbf9a5441a1002920c72b6990b445d45c2b956/pypdf-4.1.0-py3-none-any.whl.metadata\n",
      "  Downloading pypdf-4.1.0-py3-none-any.whl.metadata (7.4 kB)\n",
      "Downloading pypdf-4.1.0-py3-none-any.whl (286 kB)\n",
      "   ---------------------------------------- 0.0/286.1 kB ? eta -:--:--\n",
      "   - -------------------------------------- 10.2/286.1 kB ? eta -:--:--\n",
      "   -------- ------------------------------- 61.4/286.1 kB 1.1 MB/s eta 0:00:01\n",
      "   ------------------------------- -------- 225.3/286.1 kB 2.3 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 286.1/286.1 kB 2.5 MB/s eta 0:00:00\n",
      "Installing collected packages: pypdf\n",
      "Successfully installed pypdf-4.1.0\n"
     ]
    }
   ],
   "source": [
    "!pip install pypdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2d3e92ed",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2d3e92ed",
    "outputId": "521d7a1d-4aa0-42e9-d2d8-0fa7d0359264"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.document_loaders import UnstructuredPDFLoader, OnlinePDFLoader, PyPDFLoader, TextLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5166d759",
   "metadata": {
    "id": "5166d759"
   },
   "source": [
    "### Load your data\n",
    "\n",
    "Next let's load up some data. I've put a few 'loaders' on there which will load data from different locations. Feel free to use the one that suits you. The default one queries one of Paul Graham's essays for a simple example. This process will only stage the loader, not actually load it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b4a2d6bf",
   "metadata": {
    "id": "b4a2d6bf"
   },
   "outputs": [],
   "source": [
    "loader = PyPDFLoader(file_path=\"20_diabete residanat 15-mai-2022.pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6_3_-lQxF51c",
   "metadata": {
    "id": "6_3_-lQxF51c"
   },
   "outputs": [],
   "source": [
    "loader\n",
    "data = loader.load()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "oC8KgTp7LYEi",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "oC8KgTp7LYEi",
    "outputId": "e5559189-6bb9-4631-bb06-f684eaee1318"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8d38044",
   "metadata": {
    "id": "c8d38044"
   },
   "source": [
    "Then let's go ahead and actually load the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bcdac23c",
   "metadata": {
    "id": "bcdac23c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in c:\\users\\hachichamed\\anaconda3\\lib\\site-packages (3.8.1)\n",
      "Requirement already satisfied: click in c:\\users\\hachichamed\\anaconda3\\lib\\site-packages (from nltk) (8.0.4)\n",
      "Requirement already satisfied: joblib in c:\\users\\hachichamed\\anaconda3\\lib\\site-packages (from nltk) (1.2.0)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\hachichamed\\anaconda3\\lib\\site-packages (from nltk) (2022.7.9)\n",
      "Requirement already satisfied: tqdm in c:\\users\\hachichamed\\anaconda3\\lib\\site-packages (from nltk) (4.65.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\hachichamed\\anaconda3\\lib\\site-packages (from click->nltk) (0.4.6)\n"
     ]
    }
   ],
   "source": [
    "!pip install nltk\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "KaZqQvzuGxiY",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "KaZqQvzuGxiY",
    "outputId": "d07035ff-c1f1-4a47-8530-5132ab76b29c"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\hachichaMed\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers\\punkt.zip.\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\hachichaMed\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\stopwords.zip.\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\hachichaMed\\AppData\\Roaming\\nltk_data...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "# Download NLTK resources\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0Uz38G-ZF8fh",
   "metadata": {
    "id": "0Uz38G-ZF8fh"
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import re\n",
    "\n",
    "\n",
    "def clean_text_with_nltk(text):\n",
    "    # Tokenize the text into words\n",
    "    words = word_tokenize(text)\n",
    "\n",
    "    # Remove non-alphanumeric characters and convert to lowercase\n",
    "    clean_words = [re.sub(r'[^a-zA-Z0-9]', '', word.lower()) for word in words]\n",
    "\n",
    "    # Remove stopwords\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    clean_words = [word for word in clean_words if word not in stop_words]\n",
    "\n",
    "    # Lemmatize the words\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    clean_words = [lemmatizer.lemmatize(word) for word in clean_words]\n",
    "\n",
    "    # Join the words back into a cleaned text\n",
    "    cleaned_text = ' '.join(clean_words)\n",
    "\n",
    "    return cleaned_text\n",
    "\n",
    "\n",
    "# Clean the text extract using NLTK\n",
    "data[0].page_content = clean_text_with_nltk(data[0].page_content)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7XspMxfhMCcM",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7XspMxfhMCcM",
    "outputId": "d8550232-f19b-4e8e-e70c-bc999f394af8"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e07a744a",
   "metadata": {
    "id": "e07a744a"
   },
   "source": [
    "Then let's actually check out what's been loaded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b4fd7c9e",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "b4fd7c9e",
    "outputId": "822d5777-a456-4be5-abea-f7d45e8fa89f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You have 72 document(s) in your data\n",
      "There are 1712 characters in your sample document\n",
      "Here is a sample: cours commun de rsidanat mai 2022 1 sujet 20  diabte sucr n validation  0620202233 1 cours de rsidanat sujet  20 diabte sucr epidmiologie  etiopathognie  dpistage  diagnostic  complication  traitement\n"
     ]
    }
   ],
   "source": [
    "# Note: If you're using PyPDFLoader then it will split by page for you already\n",
    "print (f'You have {len(data)} document(s) in your data')\n",
    "print (f'There are {len(data[0].page_content)} characters in your sample document')\n",
    "print (f'Here is a sample: {data[0].page_content[:200]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8af9b604",
   "metadata": {
    "id": "8af9b604"
   },
   "source": [
    "### Chunk your data up into smaller documents\n",
    "\n",
    "While we could pass the entire essay to a model w/ long context, we want to be picky about which information we share with our model. The better signal to noise ratio we have the more likely we are to get the right answer.\n",
    "\n",
    "The first thing we'll do is chunk up our document into smaller pieces. The goal will be to take only a few of those smaller pieces and pass them to the LLM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "fb3c6f02",
   "metadata": {
    "id": "fb3c6f02"
   },
   "outputs": [],
   "source": [
    "# We'll split our data into chunks around 500 characters each with a 50 character overlap. These are relatively small.\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=50)\n",
    "texts = text_splitter.split_documents(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "zYSaS-RwNQzq",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "zYSaS-RwNQzq",
    "outputId": "4caf71dc-452f-48f9-a58e-90ffed294f48"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "879873a4",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "879873a4",
    "outputId": "a4bbf629-bb89-4bca-ad40-1880635f0950"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Now you have 477 documents\n"
     ]
    }
   ],
   "source": [
    "# Let's see how many small chunks we have\n",
    "print (f'Now you have {len(texts)} documents')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "838b2843",
   "metadata": {
    "id": "838b2843"
   },
   "source": [
    "### Create embeddings of your documents to get ready for semantic search\n",
    "\n",
    "Next up we need to prepare for similarity searches. The way we do this is through embedding our documents (getting a vector per document).\n",
    "\n",
    "This will help us compare documents later on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "L484XDOcnzXP",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "L484XDOcnzXP",
    "outputId": "01f984f4-226f-4fc2-edda-850d93f40143"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pinecone-client\n",
      "  Obtaining dependency information for pinecone-client from https://files.pythonhosted.org/packages/70/8b/f9088d60b1a13f862615b01791b8bfdf62b3fb38c0104db31005dcefa496/pinecone_client-3.2.0-py3-none-any.whl.metadata\n",
      "  Downloading pinecone_client-3.2.0-py3-none-any.whl.metadata (16 kB)\n",
      "Requirement already satisfied: certifi>=2019.11.17 in c:\\users\\hachichamed\\anaconda3\\lib\\site-packages (from pinecone-client) (2023.7.22)\n",
      "Requirement already satisfied: tqdm>=4.64.1 in c:\\users\\hachichamed\\anaconda3\\lib\\site-packages (from pinecone-client) (4.65.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4 in c:\\users\\hachichamed\\anaconda3\\lib\\site-packages (from pinecone-client) (4.10.0)\n",
      "Requirement already satisfied: urllib3>=1.26.0 in c:\\users\\hachichamed\\anaconda3\\lib\\site-packages (from pinecone-client) (1.26.16)\n",
      "Requirement already satisfied: colorama in c:\\users\\hachichamed\\anaconda3\\lib\\site-packages (from tqdm>=4.64.1->pinecone-client) (0.4.6)\n",
      "Downloading pinecone_client-3.2.0-py3-none-any.whl (213 kB)\n",
      "   ---------------------------------------- 0.0/213.6 kB ? eta -:--:--\n",
      "   - -------------------------------------- 10.2/213.6 kB ? eta -:--:--\n",
      "   ----------------- ---------------------- 92.2/213.6 kB 1.3 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 213.6/213.6 kB 1.9 MB/s eta 0:00:00\n",
      "Installing collected packages: pinecone-client\n",
      "Successfully installed pinecone-client-3.2.0\n"
     ]
    }
   ],
   "source": [
    "!pip install pinecone-client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "373e695a",
   "metadata": {
    "id": "373e695a"
   },
   "outputs": [],
   "source": [
    "from langchain.vectorstores import Chroma, Pinecone\n",
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "import pinecone"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "884e7857",
   "metadata": {
    "id": "884e7857"
   },
   "source": [
    "Check to see if there is an environment variable with you API keys, if not, use what you put below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "42a1d5c3",
   "metadata": {
    "hide_input": false,
    "id": "42a1d5c3"
   },
   "outputs": [],
   "source": [
    "OPENAI_API_KEY = \"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\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "K76CrKjpoX6E",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "K76CrKjpoX6E",
    "outputId": "1857f5c9-b2e3-43bd-b0ef-a754f0d598fa"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: openai in c:\\users\\hachichamed\\anaconda3\\lib\\site-packages (1.13.3)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in c:\\users\\hachichamed\\anaconda3\\lib\\site-packages (from openai) (3.5.0)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in c:\\users\\hachichamed\\anaconda3\\lib\\site-packages (from openai) (1.9.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in c:\\users\\hachichamed\\anaconda3\\lib\\site-packages (from openai) (0.27.0)\n",
      "Requirement already satisfied: pydantic<3,>=1.9.0 in c:\\users\\hachichamed\\anaconda3\\lib\\site-packages (from openai) (1.10.8)\n",
      "Requirement already satisfied: sniffio in c:\\users\\hachichamed\\anaconda3\\lib\\site-packages (from openai) (1.2.0)\n",
      "Requirement already satisfied: tqdm>4 in c:\\users\\hachichamed\\anaconda3\\lib\\site-packages (from openai) (4.65.0)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.7 in c:\\users\\hachichamed\\anaconda3\\lib\\site-packages (from openai) (4.10.0)\n",
      "Requirement already satisfied: idna>=2.8 in c:\\users\\hachichamed\\anaconda3\\lib\\site-packages (from anyio<5,>=3.5.0->openai) (3.4)\n",
      "Requirement already satisfied: certifi in c:\\users\\hachichamed\\anaconda3\\lib\\site-packages (from httpx<1,>=0.23.0->openai) (2023.7.22)\n",
      "Requirement already satisfied: httpcore==1.* in c:\\users\\hachichamed\\anaconda3\\lib\\site-packages (from httpx<1,>=0.23.0->openai) (1.0.4)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in c:\\users\\hachichamed\\anaconda3\\lib\\site-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.14.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\hachichamed\\anaconda3\\lib\\site-packages (from tqdm>4->openai) (0.4.6)\n"
     ]
    }
   ],
   "source": [
    "!pip install openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "64oEJdc8oQpp",
   "metadata": {
    "id": "64oEJdc8oQpp"
   },
   "outputs": [],
   "source": [
    "from openai import OpenAI\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3205993a",
   "metadata": {
    "id": "3205993a"
   },
   "source": [
    "Then we'll get our embeddings engine going. You can use whatever embeddings engine you would like. We'll use OpenAI's ada today."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "b4619d3a",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "b4619d3a",
    "outputId": "803773c7-2cf0-455d-9ea9-69dcff99591b"
   },
   "outputs": [],
   "source": [
    "embeddings = OpenAIEmbeddings(openai_api_key=OPENAI_API_KEY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "Qt2YnQKs9Zx0",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Qt2YnQKs9Zx0",
    "outputId": "fc8c4bd3-0ba3-46d9-f63f-dff347c360ad"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tiktoken in c:\\users\\hachichamed\\anaconda3\\lib\\site-packages (0.6.0)\n",
      "Requirement already satisfied: regex>=2022.1.18 in c:\\users\\hachichamed\\anaconda3\\lib\\site-packages (from tiktoken) (2022.7.9)\n",
      "Requirement already satisfied: requests>=2.26.0 in c:\\users\\hachichamed\\anaconda3\\lib\\site-packages (from tiktoken) (2.31.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\hachichamed\\anaconda3\\lib\\site-packages (from requests>=2.26.0->tiktoken) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\hachichamed\\anaconda3\\lib\\site-packages (from requests>=2.26.0->tiktoken) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\hachichamed\\anaconda3\\lib\\site-packages (from requests>=2.26.0->tiktoken) (1.26.16)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\hachichamed\\anaconda3\\lib\\site-packages (from requests>=2.26.0->tiktoken) (2023.7.22)\n"
     ]
    }
   ],
   "source": [
    "!pip install tiktoken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "-SRGScFJKw5J",
   "metadata": {
    "id": "-SRGScFJKw5J"
   },
   "outputs": [],
   "source": [
    "from pinecone import Pinecone\n",
    "from langchain.vectorstores import Pinecone as PineconeStore\n",
    "import os\n",
    "pc = Pinecone(\"dcf2b455-fcca-4cf4-8ffa-834e30214e41\")\n",
    "index = pc.Index(\"diab\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "0e093ef3",
   "metadata": {
    "hide_input": false,
    "id": "0e093ef3"
   },
   "outputs": [],
   "source": [
    "docsearch=PineconeStore.from_texts(\n",
    "    [t.page_content for t in texts],\n",
    "    embeddings,\n",
    "    index_name=\"diab\"\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
